Title: "[Data Science 到底是什麼從一個完全外行角度來看][10]用.Net Core跑Hadoop MapReduce - Streaming介紹"
Published: 2017-12-30
Modified: 2018-02-10
Image: /posts/migrate/2017-12-30-data-science-series-10-hadoop-streaming-intro-use-net-core-for-MapReduce_Asset/image_thumb(69).png
Tags: ["docker","hadoop","「data science 到底是什麼從一個完全外行角度來看」","data science","net-core"]
RedirectFrom: 2017/12/data-science-series-10-hadoop-streaming-intro-use-net-core-for-MapReduce.html
Series: ["「Data Science 到底是什麼從一個完全外行角度來看」"]
---
<section><figure><a href="https://lh3.googleusercontent.com/-RP0Gkq3i-oE/WkcnGQeRhAI/AAAAAAAAXW0/HgFyGTkmNwE97vofL36E-suboOHJluFSQCHMYCw/s1600-h/image%255B2%255D"><img width="654" height="290" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-30-data-science-series-10-hadoop-streaming-intro-use-net-core-for-MapReduce_Asset/image_thumb(69).png" border="0" class="img-responsive"></a><br><figcaption><span>圖片來源：                 <a href="https://pixabay.com/en/books-spine-colors-pastel-1099067/">https://pixabay.com/en/books-spine-colors-pastel-1099067/</a> 和                 <a href="https://pixabay.com/en/math-blackboard-education-classroom-1547018/">https://pixabay.com/en/math-blackboard-education-classroom-1547018/</a></span></figcaption></figure></section><section><p>上一篇(<a href="http://blog.alantsai.net/2017/12/data-science-series-09-hadoop-map-reduce-java-wordcount-example.html">[09]了解Hadoop裡的MapReduce到底是什麼？</a>)了解了什麼是MapReduce，並且了解了怎麼用Java寫一個MapReduce的Hello World程式：WordCount。             </p><p>馬上會想到的一個問題是，難道只有Java可以寫MapReduce的程式嗎？             </p><p>這篇將會介紹Hadoop的Streaming服務，讓任何語言只要透過<em>Standard Input和Standard Output</em>就可以寫出MapReduce程式。                 將會使用最熟悉的語言，.Net Core來完成這個事情。             </p><p>在這篇也會介紹另外一種測試Hadoop的方式，使用Docker來測試。             </p><div class="bs-callout bs-callout-default">這篇的範例程式碼在github：<a href="https://github.com/alantsai/blog-data-science-series/tree/master/src/chapter-10-dotnet-mapreduce">alantsai/blog-data-science-series 裡面的 src/chapter-10-dotnet-mapreduce</a></div></section>                 <section> <a name="KMContentPageTopID" id="KMContentPageTopID"></a><div id="divKMOutline" style="border-style: groove none; margin: 10px 0px;"><ul style="margin: 0px 0px 0px 20px;"><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514612012655599" ;="">什麼是Hadoop Streaming</a><br></li><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514612012656732" ;="">實際操作</a><br></li><ul style="margin: 0px 0px 0px 30px;"><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514612012656715" ;="">Mapper開發</a><br></li><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514612012656712" ;="">Reducer開發</a><br></li><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514612012656716" ;="">測試結果</a><br></li></ul><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514612012656878" ;="">結語</a><br></li></ul></div></section>                  <a name="more"></a><section><h2 id="WizKMOutline_1514612012655599">什麼是Hadoop Streaming</h2><p>當一個MapReduce的程式被執行的時候，會先被切割成為一個一個的Task，然後由那台的DataNode用Java執行那個Task。             </p><p>所以整個執行類似下圖，整個MapReduce都在JVM的環境下：                 </p><figure><a href="https://lh3.googleusercontent.com/-nllf8H6fyGA/WkcnIu6LLaI/AAAAAAAAXW8/TRgczukT6H8k7LcBlUe-vrO6WQrQfs8NACHMYCw/s1600-h/image%255B5%255D"><img width="654" height="369" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-30-data-science-series-10-hadoop-streaming-intro-use-net-core-for-MapReduce_Asset/image_thumb[1](59).png" border="0" class="img-responsive"></a><br><figcaption>JVM的MapReduce</figcaption></figure><p>不過Hadoop考量到如果外部需要執行MapReduce要怎麼辦，因此建立了一個叫做Streaming的功能。             </p><p>基本上，只要那台DataNode可以<em><strong>Run的起來</strong></em>都可以跑。             </p><p>Hadoop Streaming透過<em><strong>Standard Input/Output/Error 3個管道</strong></em> 來和被Run起來的程式溝通。             </p><p>MapReduce的程式只需要從Standard Input讀進來，做處理，然後在寫到Output。如果有錯誤訊息可以記錄在Error裡面。             </p><p>整個概念大概是：                 </p><figure><a href="https://lh3.googleusercontent.com/-7aZ4PVZ1mEw/WkcwowUSYSI/AAAAAAAAXYQ/jeYjCaZDnqoYTJBUUPwdmFQHHr3Otv0TACHMYCw/s1600-h/image%255B3%255D"><img width="654" height="482" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-30-data-science-series-10-hadoop-streaming-intro-use-net-core-for-MapReduce_Asset/image_thumb[1](60).png" border="0" class="img-responsive"></a><br><figcaption>Hadoop Streaming</figcaption></figure></section><section><h2 id="WizKMOutline_1514612012656732">實際操作</h2><p>還記得整個MapReduce基本上就是在每個階段做過處理之後，會產生一個key value pair。Hadoop用<em><strong>tab</strong></em>來切割Key 和 Value。             </p><p>有了這個概念之後來看實際程式，以下使用的是.Net Core的console來開發，分幾個階段：                 </p><ol><li>Mapper開發</li><li>Reducer開發</li><li>測試結果</li></ol><section><h3 id="WizKMOutline_1514612012656715">Mapper開發</h3><p>由於是透過Standard Input/Output，因此console非常適合，所以會建立一個Mapper的.Net Core Console程式。                </p><p>在Mapper的階段，內容會是一行一行讀進來，所以把讀進來的內容做文字切割，                     每找到一個word，就寫到output，<em>word是key</em>，<em>1是value</em>（代表找到一筆）                     </p><p>會一直迴圈的讀，直到沒有任何檔案為止。如果把這個和之前java比照會發現邏輯一樣。                 </p><pre class="brush: csharp;"><code class="language-csharp line-numbers">class Program
{
    static void Main(string[] args)
    {
        string line;

        while ((line = Console.ReadLine()) != null)
        {
            // 用文字切割
            var words = Regex.Matches(line, @@"[\w]+");

            foreach (var word in words)
            {
                // 每一個找到的算1筆 - keyvalue用tab切割
                Console.WriteLine("{0}\t1", word);
            }
        }
    }
}</code></pre></section><section><h3 id="WizKMOutline_1514612012656712">Reducer開發</h3><p>會在建立另外一個專案用來放Reducer的程式。                 </p><p>Reducer一樣是讀Input然後寫到output。由於這次讀到的內容<em><strong>是從Mapper來的</strong></em>，所以會先用tab做切割，key是word，value就是筆數（也都是1）。                 </p><p>在這邊，有建立一個words dictionary，這個是因為在Mapper階段其實<em>沒有管</em>word有沒有重複，反正出現就是+1。                 </p><p>不過在Reducer因為要加總，因此用了<code>words</code> dictionary作為一個暫存的空間。                 </p><p>最後把所有結果寫到output - 也是 key value pair，key一樣是word，不過value就是word出現的總數。                 </p><pre class="brush: csharp;"><code class="language-csharp line-numbers">static void Main(string[] args)
{
    // 用來儲存已經出現過的字 - java版本會自動處理，不過這個stream需要手動記錄
    Dictionary&lt;string, int&gt; words = new Dictionary&lt;string, int&gt;();

    string line;

    while ((line = Console.ReadLine()) != null)
    {
        // 傳過來的key value用tab分割（Mapper也是用tab切割key和value）
        var keyValuePair = line.Split('\t');

        string word = keyValuePair[0];

        int count = Convert.ToInt32(keyValuePair[1]);

        // 如果已經有這個word，和字典的加總，不然就建立新的
        if (words.ContainsKey(word))
        {
            words[word] += count;
        }
        else
        {
            words.Add(word, count);
        }
    }

    // 把所有結果寫出來
    foreach (var word in words)
    {
        Console.WriteLine("{0}\t{1}", word.Key, word.Value);
}</code></pre><div class="bs-callout bs-callout-info">和Java的版本不同，java版本會自動幫忙把key一樣組成一個list比較好操作，但是透過streaming需要自己手動操作。                 </div></section><section><h3 id="WizKMOutline_1514612012656716">測試結果</h3><p>當整個程式準備好了之後，接下來就可以對這個程式做測試了。                 </p><p>在接下來將會用一個docker版本的hadoop做測試 - 希望透過docker方式也可以了解用docker做測試有多方便。                 </p><p>接下來的測試都是在powershell可以直接執行。                 </p><div class="bs-callout bs-callout-info"><p>如果對docker不熟悉，那麼下面做不了。要跑docker基本上要Windows 10 Professional以上或者linux，並且有裝docker。                     </p><p>裡面用到的docker image是一個linux的container。                     </p><p>下面也可以直接在之前建立的Ubuntu環境裡面執行，不過需要先：                         </p><ul><li>                             安裝.net core 2.0                             </li><li>                                 跳過前面的步奏，知道後面呼叫hadoop Streaming那段即可                             </li></ul></div><div class="bs-callout bs-callout-info"><p>接下來的指令操作都是在從github clone下來的專案裡面<code>src\chapter-10-dotnet-mapreduce</code>的資料夾下面執行。                     </p><p>完整的指令是：                         </p><pre class="brush: powershell;"><code class="language-powershell line-numbers">git clone https://github.com/alantsai/blog-data-science-series.git
cd .\blog-data-science-series\src\chapter-10-dotnet-mapreduce
                        </code></pre></div><dl><dt>先把.net core的console 發佈出來</dt><dd>                         在powershell執行指令：<code>dotnet publish -o ${pwd}\dotnetmapreduce .\DotNetMapReduceWordCount\DotNetMapReduceWordCount.sln</code><figure><a href="https://lh3.googleusercontent.com/-7hjaKXRXqo8/WkcnNEXGaXI/AAAAAAAAXXM/hwtIMKEwJJcDVJSmiyr9Ly5lLCjqF0QHACHMYCw/s1600-h/image%255B11%255D"><img width="654" height="171" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-30-data-science-series-10-hadoop-streaming-intro-use-net-core-for-MapReduce_Asset/image_thumb[3](47).png" border="0" class="img-responsive"></a> <figcaption>發佈結果</figcaption></figure></dd><dt>把hadoop用docker compose啟動</dt><dd>                         使用指令把hadoop啟動：<code>docker-compose up -d</code>。                         會看到：                         <ol><li>執行完有1個master 2個worker啟動</li><li>在YARN的web節點看到有兩個Node</li><li>在DataNode看到有兩個節點</li></ol><figure><a href="https://lh3.googleusercontent.com/-aVz_W4CVO0Q/WkcnPZd6SSI/AAAAAAAAXXU/OHhWZylncDYH90c0VNdssPiBWdaaNatwACHMYCw/s1600-h/image%255B14%255D"><img width="654" height="365" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-30-data-science-series-10-hadoop-streaming-intro-use-net-core-for-MapReduce_Asset/image_thumb[4](47).png" border="0" class="img-responsive"></a> <figcaption>可以看到啟動成功並且有兩個節點</figcaption></figure></dd><dt>把.Net core程式複製到master的hadoop節點裡面</dt><dd>                         把剛剛發佈出來的.Net core程式複製到master裡面，並且進入到master裡面的bash並且可以看到有copy進去的內容                         <pre class="brush: powershell;"><code class="language-powershell line-numbers">docker cp dotnetmapreduce hadoop-dotnet-master:/dotnetmapreduce
docker exec -it hadoop-dotnet-master bash
ls
ls /dotnetmapreduce</code></pre><figure><a href="https://lh3.googleusercontent.com/-15cJ7kHjBNw/WkcnSG-pnjI/AAAAAAAAXXc/BTRexfbAh7clmnNFRYw6NRazwCkhHT60wCHMYCw/s1600-h/image%255B17%255D"><img width="654" height="171" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-30-data-science-series-10-hadoop-streaming-intro-use-net-core-for-MapReduce_Asset/image_thumb[5](32).png" border="0" class="img-responsive"></a><figcaption>進入到master的bash並且檢查copy是否成功</figcaption></figure></dd><dt>把要計算的檔案放到hadoop的HDFS</dt><dd>                         透過下面指令把檔案放到hadoop的HDFS的input資料夾並且檢查：                         <pre class="brush: powershell;"><code class="language-powershell line-numbers">hadoop fs -mkdir -p /input
hadoop fs -copyFromLocal /dotnetmapreduce/jane_austen.txt /input
hadoop fs -ls /input
                        </code></pre><figure><a href="https://lh3.googleusercontent.com/-23RRtJuVDCQ/WkcnUwQkH3I/AAAAAAAAXXk/OW3hTR1s_YwGvMIH3xnlNQJBc3FU8Y48ACHMYCw/s1600-h/image%255B20%255D"><img width="654" height="120" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-30-data-science-series-10-hadoop-streaming-intro-use-net-core-for-MapReduce_Asset/image_thumb[6](31).png" border="0" class="img-responsive"></a> <figcaption>複製檔案到HDFS</figcaption></figure></dd><dt>用hadoop Streaming執行net core mapreduce</dt><dd>                         用hadoop的streaming執行：                         <pre class="brush: powershell;"><code class="language-powershell line-numbers">hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \
    -files "/dotnetmapreduce" \
    -mapper "dotnet dotnetmapreduce/DotNetMapReduceWordCount.Mapper.dll" \
    -reducer  "dotnet dotnetmapreduce/DotNetMapReduceWordCount.Reducer.dll" \
    -input /input/* -output /output
                        </code></pre><figure><a href="https://lh3.googleusercontent.com/-RYjrgOTeGK0/WkcnYo6-N8I/AAAAAAAAXXs/0bZQKz6SVPgPCmZIi2_FRfb1_o39HwcAQCHMYCw/s1600-h/image%255B23%255D"><img width="654" height="223" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-30-data-science-series-10-hadoop-streaming-intro-use-net-core-for-MapReduce_Asset/image_thumb[7](25).png" border="0" class="img-responsive"></a> <figcaption>和之前執行map reduce的log一樣</figcaption></figure></dd><dt>檢查結果</dt><dd>                         執行完了之後，可以看到計算的每個字出現次數 <pre class="brush: powershell;"><code class="language-powershell line-numbers">hadoop fs -ls /output
hadoop fs -cat /output/part-00000
</code></pre><figure><a href="https://lh3.googleusercontent.com/-RhzE6QlyhKw/Wkcna6qZwKI/AAAAAAAAXX0/QFpyLLHbJcMedKAEtOkyf9EHowi4mplpQCHMYCw/s1600-h/image%255B26%255D"><img width="654" height="413" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-30-data-science-series-10-hadoop-streaming-intro-use-net-core-for-MapReduce_Asset/image_thumb[8](23).png" border="0" class="img-responsive"></a> <figcaption>執行結果</figcaption> </figure><div class="bs-callout bs-callout-warning">會注意到這邊的結果和java版本有點不同，因為判斷字的邏輯不同導致。                         </div><div class="bs-callout bs-callout-info">如果docker不需要了，可以用<code>docker-compose down</code>把整個hadoop關掉。                         </div></dd></dl></section></section><section><h2 id="WizKMOutline_1514612012656878">結語</h2><p>在這篇介紹了透過Hadoop Streaming達到在hadoop用.Net core 2.0的console程式做MapReduce如何。             </p><p>這篇也改成使用docker來做hadoop測試而不是用一直以來建立的VM。用docker和VM比較會發現到docker其實做這種事情非常方便，如果對docker不熟悉，可以考慮花點時間做些學習（之後我的部落格也會有個系列介紹docker使用，有興趣的話請持續關注）。             </p><div class="bs-callout bs-callout-info">在這個系列的後面，之前建立的VM還會用到 - 用來和R做結合。所以如果對後面操作有興趣，VM還是先保留。             </div><p>在這個系列的Hadoop介紹也到了一個尾聲，在下一篇(<a href="http://blog.alantsai.net/2017/12/data-science-series-11-hadoop-conclustion-ecosystem-intro.html">[11]Hadoop總結(上篇)–Ecosystem介紹</a>)將會對目前hadoop有介紹的部分做一個總結，介紹hadoop的ecosystem，和還有什麼部分是應該繼續關注下去。             </p></section>             <section>                 <div class="wlWriterEditableSmartContent" id="scid:77ECF5F8-D252-44F5-B4EB-D463C5396A79:85f26ad6-2c99-4be6-9f2d-59dc57c01179" style="margin: 0px; padding: 0px; float: none; display: inline;">標籤: <a href="/tags/%e3%80%8cData+Science+%e5%88%b0%e5%ba%95%e6%98%af%e4%bb%80%e9%ba%bc%e5%be%9e%e4%b8%80%e5%80%8b%e5%ae%8c%e5%85%a8%e5%a4%96%e8%a1%8c%e8%a7%92%e5%ba%a6%e4%be%86%e7%9c%8b%e3%80%8d" rel="tag">「Data Science 到底是什麼從一個完全外行角度來看」</a>,<a href="/tags/data+science" rel="tag">data science</a>,<a href="/tags/hadoop" rel="tag">hadoop</a>,<a href="/tags/net-core" rel="tag">net-core</a>,<a href="/tags/docker" rel="tag">docker</a></div></section>