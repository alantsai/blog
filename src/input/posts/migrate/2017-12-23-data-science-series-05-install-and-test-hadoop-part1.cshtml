Title: "[Data Science 到底是什麼從一個完全外行角度來看][05]建立Hadoop環境 -上篇"
Published: 2017-12-23
Modified: 2017-12-24
Image: /posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb(64).png
Tags: ["hadoop","「data science 到底是什麼從一個完全外行角度來看」","data science"]
RedirectFrom: 2017/12/data-science-series-05-install-and-test-hadoop-part1.html
Series: ["「Data Science 到底是什麼從一個完全外行角度來看」"]
---
<section><figure><a href="https://lh3.googleusercontent.com/-TVWd5UeyrC8/Wj47ExLZKvI/AAAAAAAAXMg/jRP-EzPHFesU9CLaix1S4YpkH6n8L7nnQCHMYCw/s1600-h/image%255B2%255D"><img width="654" height="290" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb(64).png" border="0" class="img-responsive"></a><br><figcaption><span>圖片來源：<a href="https://pixabay.com/en/books-spine-colors-pastel-1099067/">https://pixabay.com/en/books-spine-colors-pastel-1099067/</a> 和 <a href="https://pixabay.com/en/math-blackboard-education-classroom-1547018/">https://pixabay.com/en/math-blackboard-education-classroom-1547018/</a></span></figcaption><br></figure></section><section><p>上一篇（<a href="http://blog.alantsai.net/2017/12/data-science-series-04-hadoop-intro.html">[04]Hadoop是什麼？</a>）以一個非常高的overview看了Hadoop是什麼，在接下來將會把理論轉成實際操作，將建立一個Ubuntu 的 VM上面架設hadoop並且跑一個MapReduce的hello world程式，WordCount（算字數）。</p><p>等到跑完範例之後，將會在深入一點看hadoop的MapReduce和HDFS運作模式。   </p><p>首先，從建立環境開始。   </p></section>   <section> <a name="KMContentPageTopID" id="KMContentPageTopID"></a><div id="divKMOutline" style="border-style: groove none; margin: 10px 0px;"><ul style="margin: 0px 0px 0px 20px;"><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514026901216988" ;="">環境準備</a><br></li><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514026901223166" ;="">建立Hadoop測試環境</a><br></li><ul style="margin: 0px 0px 0px 30px;"><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514026901223379" ;="">安裝Ubuntu VM</a><br></li><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514026901223970" ;="">設定Ubuntu環境</a><br></li></ul><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514026901223140" ;="">結語</a><br></li></ul></div></section>    <a name="more"></a> <section><h2 id="WizKMOutline_1514026901216988">環境準備</h2><p>整個操作會是在VM（虛擬機器）上面執行，並且因為Hadoop在linux世界比在Windows來的穩定，因此，會建立一個Ubuntu的環境，並且把Hadoop架設在裡面。   </p><p>在接下來的lab將會用到以下幾個軟體/環境：   </p><dl><dt>     主機環境    </dt><dd>     接下來使用到的機器規格如下：     <ul><li>OS - Windows 10 1703</li><li>CPU - i7-6500U 雙核</li><li>Memory - 16GB</li></ul></dd><dt>     VMWare Player 14    </dt><dd><p>任何虛擬機器軟體都可以，只是剛好用的是VMWare Player 14。    </p><ul><li><a href="https://my.vmware.com/en/web/vmware/free#desktop_end_user_computing/vmware_workstation_player/14_0|PLAYER-1410|product_downloads">下載頁面</a></li><li>檔案大小約 90MB </li></ul></dd><dt>     Ubuntu 16.04.3    </dt><dd><p>其他版本的Ubuntu也沒問題 - 如果用的是Ubuntu 14，那麼只有等一下安裝openjdk的部分會有問題，其他都一樣。     </p><ul><li><a href="https://www.ubuntu.com/download/desktop">下載頁面</a></li><li><a href="https://www.ubuntu.com/download/desktop/thank-you?version=16.04.3&amp;architecture=amd64">直接下載（約1.4GB）</a></li></ul></dd><dt>     Hadoop v2.7.4    </dt><dd><p>基本上 v2.x 的都沒有問題，只是剛好手上有2.7.4所以沒有在下載新的。如果是v3.0那麼設定會不同     </p><ul><li><a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz">下載頁面</a></li><li><a href="http://apache.stu.edu.tw/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz">直接下載（約254 MB）</a></li></ul></dd><dt>     MapReduce的Hello World程式 - WordCount    </dt><dd><p>這個是用來測試map reduce的hello world程式：     </p><ul><li><a href="https://1drv.ms/u/s!Am-7OXCiIo1Lh9EaLbZsQyArJyYZ6Q">WordCount2.jar</a></li><li><a href="https://1drv.ms/t/s!Am-7OXCiIo1Lh9EbxkNiZl1d7ERYtQ">jane_austen.txt</a> - pride and prejudice 前三章 - 測試算字數用</li></ul></dd></dl><p>以上就是整個會用到的程式和環境，接下來就來看看如何建立hadoop環境。   </p></section><section><h2 id="WizKMOutline_1514026901223166">建立Hadoop測試環境</h2><p>基本上整個的環境建立大概可以分幾個部分：   </p><ol><li>安裝Ubuntu VM</li><li>設定Ubuntu環境</li><li>安裝和設定Hadoop</li><li>測試Hadoop</li></ol><div class="bs-callout bs-callout-info">由於截圖比較多，所以這篇會先介紹<em>第一步和第二部</em>的部分，hadoop安裝和測試將會在下一篇做介紹   </div><section><h3 id="WizKMOutline_1514026901223379">安裝Ubuntu VM</h3><p>首先先把VMWare Player安裝起來（<a href="https://my.vmware.com/en/web/vmware/free#desktop_end_user_computing/vmware_workstation_player/14_0|PLAYER-1410|product_downloads">下載頁面</a>）    </p><p>把VMWare Player執行起來，先建立VM：    </p><figure><a href="https://lh3.googleusercontent.com/-75cWRoboxMU/Wj47HWFFhBI/AAAAAAAAXMo/c2pHdFg7F3k0UCcmXNzN4pJ68mnckJpQQCHMYCw/s1600-h/image%255B5%255D"><img width="654" height="253" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb[1](56).png" border="0" class="img-responsive"></a><br><figcaption>建立VM</figcaption></figure><p>選擇下載的Ubuntu iso檔案位置（<a href="https://www.ubuntu.com/download/desktop/thank-you?version=16.04.3&amp;architecture=amd64">直接下載</a>）    </p><figure><a href="https://lh3.googleusercontent.com/-0EGERhxXBF4/Wj47JC6gtsI/AAAAAAAAXMw/33_54YZ54koP-g9NoA5VI1IdewbiMvpjACHMYCw/s1600-h/image%255B8%255D"><img width="645" height="638" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb[2](46).png" border="0" class="img-responsive"></a><br><figcaption>選擇iso檔案的路徑</figcaption></figure><p>設定帳號的部分，建議設定<code>hduser</code>，如果設定不同，在下面的修改需要作出對應修改。    </p><figure><a href="https://lh3.googleusercontent.com/-yoCvFbfeMbE/Wj47LztRqtI/AAAAAAAAXM4/HtZQXE7HyFsPAvCSwdB6Mz5uEJKbikKpgCHMYCw/s1600-h/image%255B11%255D"><img width="645" height="638" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb[3](43).png" border="0" class="img-responsive"></a><br><figcaption>帳號設定畫面</figcaption></figure><p>機器的名稱和儲存位置就隨意，只要可以識別即可    </p><figure><a href="https://lh3.googleusercontent.com/-Dtd3sJf9FbE/Wj47O5FadZI/AAAAAAAAXNA/9vcPTTm2Arc5qMRXBOyHjVrxkTGuEyuoQCHMYCw/s1600-h/image%255B14%255D"><img width="645" height="638" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb[4](43).png" border="0" class="img-responsive"></a><br><figcaption>VM名稱</figcaption></figure><p>VM硬碟的部分，20GB不用動，下面那個選項建議改成<code>第一個選項</code>，原因是之後要複製比較方便。    </p><figure><a href="https://lh3.googleusercontent.com/-m8nx_bLBxw8/Wj47Sr054xI/AAAAAAAAXNI/lSg3ZF8qkvE88sOyYDWBmdxn6fF2yvJ4ACHMYCw/s1600-h/image%255B17%255D"><img width="645" height="638" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb[5](29).png" border="0" class="img-responsive"></a><br><figcaption>設定硬碟</figcaption></figure><p>設定CPU和memory的部分需要透過：    </p><ol><li>選擇 <kbd>Customize Hardware</kbd></li><li>選擇 Memory</li><li>最好至少要有<em>2048MB</em></li><li>按下<kbd>close</kbd>即可，修改就會儲存</li></ol><div class="bs-callout bs-callout-info">最好 CPU 能夠給到 <em>2+</em>，Memory最好可以到<em>4096 MB+</em> - 後面執行比較不會有問題（要不然需要在手動調整一些使用資源避免執行不起來）。    </div><figure><a href="https://lh3.googleusercontent.com/--4-cvhsK6rI/Wj47VLHu9KI/AAAAAAAAXNQ/KvIs4EXS0qUki6xefH7OPIX2o1KEsWeLgCHMYCw/s1600-h/image%255B20%255D"><img width="654" height="356" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb[6](27).png" border="0" class="img-responsive"></a><br><figcaption>設定資源</figcaption></figure><p>接下來VMWare Player會自動安裝，如果有出現要不要安裝 <code>VMWare Tool for linux</code>，建議裝    </p><figure><a href="https://lh3.googleusercontent.com/-X9TtiV6yT9M/Wj47ajXpjNI/AAAAAAAAXNY/-jb1D2CY7zA8WYB-kmyYBa505FuZFeGyACHMYCw/s1600-h/image%255B23%255D"><img width="654" height="423" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb[7](21).png" border="0" class="img-responsive"></a><br><figcaption>安裝畫面</figcaption></figure><p>最後，安裝好之後，出現的就是登入畫面，直接輸入當初設定的密碼即可。    </p><figure><a href="https://lh3.googleusercontent.com/-oF8DiaFS-Eo/Wj47dVKNw8I/AAAAAAAAXNg/GmXeWgWmdQEv2Q-XgN2QnGSw0-aqeLvuwCHMYCw/s1600-h/image%255B26%255D"><img width="654" height="633" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb[8](19).png" border="0" class="img-responsive"></a><br><figcaption>登入系統</figcaption></figure></section><section><h3 id="WizKMOutline_1514026901223970">設定Ubuntu環境</h3><p>Hadoop是java的程式，因此需要先安裝Java - 正常來說Java 7就夠了，不過這邊會裝Java 8。    </p><p>再來，要設定一些環境參數讓後面用到。    </p><p>最後，會需要安裝ssh，因為啟動服務的時候會用ssh來溝通避免需要一台一台去啟動服務。    </p><dl><dt>      開啟Terminal     </dt><dd>      登入Ubuntu之後，開啟Terminal（快速鍵 <kbd>Ctrl + Alt + t</kbd>）。基本上後面會一直用到，所以記得這個快速鍵     </dd><dt>      更新package     </dt><dd><p>先更新目前package的情況，使用指令：<code>sudo apt-get update</code></p><figure><a href="https://lh3.googleusercontent.com/-4LD_udQ0wSY/Wj47lAbgXZI/AAAAAAAAXNo/Lf7CcfHgjyMKgQiJXplKJpkCM6UnsXLGACHMYCw/s1600-h/image%255B29%255D"><img width="654" height="358" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb[9](9).png" border="0" class="img-responsive"></a> <figcaption>update畫面</figcaption></figure></dd><dt>      安裝Java 8     </dt><dd>      在terminal執行：<code>sudo apt-get install openjdk-8-jre openjdk-8-jdk</code><figure><a href="https://lh3.googleusercontent.com/-1tuKsdQ4n24/Wj47pMPqG1I/AAAAAAAAXN0/ivqNko7vUpkRHzDbCjkbXq_JcitZ0W-6wCHMYCw/s1600-h/image%255B32%255D"><img width="654" height="478" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb[10](7).png" border="0" class="img-responsive"></a> <figcaption>中間有需要<em>輸入 y</em> 才會繼續執行</figcaption></figure></dd><dt>      設定環境參數     </dt><dd><p>現在terminal執行：<code>gedit ~/.bashrc</code>，然後在檔案最後面加上：      </p><pre class="brush: plain;"><code class="language-none line-numbers">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH</code></pre><figure><a href="https://lh3.googleusercontent.com/-mU1U2HUIrcY/Wj47rFkgC3I/AAAAAAAAXN8/b3OmbrQTmb8eMi6_X2IBuzmkOv8ke6aDwCHMYCw/s1600-h/image%255B35%255D"><img width="654" height="266" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb[11](10).png" border="0" class="img-responsive"></a> <figcaption>設定畫面</figcaption></figure><p>最後確認一下參數有沒有進入：      </p><pre class="brush: plain;"><code class="language-none line-numbers">source ~/.bashrc
echo $JAVA_HOME
java -version</code></pre><figure><a href="https://lh3.googleusercontent.com/-zKIBTxFQhPU/Wj47tsDDFlI/AAAAAAAAXOE/WiK62Y1hFU0wV_jmWwyU1CYLDbPI40EHACHMYCw/s1600-h/image%255B38%255D"><img width="654" height="302" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb[12](12).png" border="0" class="img-responsive"></a> <figcaption>參數確認，並且java版本是1.8</figcaption></figure></dd><dt>      安裝ssh server     </dt><dd><p>為了能夠讓master和多個slave溝通，需要安裝ssh，再來設定ssh的key：      </p><pre class="brush: plain;"><code class="language-none line-numbers">sudo apt-get install openssh-server
cd ~/.ssh/
su - hduser
ssh-keygen -t rsa</code></pre><p>在產生key的部分，正常是<em>要設定一個密碼</em>比較安全，不過這個是測試用，所以就<kbd>enter</kbd>3次下去即可      </p><figure><a href="https://lh3.googleusercontent.com/-mM2yCusWHmI/Wj47wwyJZVI/AAAAAAAAXOM/twIlhAWJhCALkohOO00ccXu2vxdDNwZKQCHMYCw/s1600-h/image%255B41%255D"><img width="616" height="466" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb[13](10).png" border="0" class="img-responsive"></a> <figcaption>產生key的畫面</figcaption></figure><p>key產生了之後，要把它寫出來並且測試ssh是否正常：      </p><pre class="brush: plain;"><code class="language-none line-numbers">cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys
ssh localhost
exit</code></pre><figure><a href="https://lh3.googleusercontent.com/-Oc8FBVwDCWA/Wj47zQaBuMI/AAAAAAAAXOU/8SdXx91YWMI-T8J1oiNaqF3zf3JrYAXhwCHMYCw/s1600-h/image%255B44%255D"><img width="654" height="707" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-23-data-science-series-05-install-and-test-hadoop-part1_Asset/image_thumb[14](6).png" border="0" class="img-responsive"></a> <figcaption>測試和退出的畫面</figcaption></figure></dd></dl></section></section><section><h2 id="WizKMOutline_1514026901223140">結語</h2><p>這篇介紹了建制測試環境的一些設定，由於圖片比較多因此把後半段hadoop的安裝/設定和測試放在下一篇。   </p><p>在下一篇(<a href="http://blog.alantsai.net/2017/12/data-science-series-06-install-and-test-hadoop-part2.html">[06]建立Hadoop環境 -下篇</a>)將會接著這篇目前VM的情況，並且把整個hadoop建制完成。   </p></section><section>   <div class="wlWriterEditableSmartContent" id="scid:77ECF5F8-D252-44F5-B4EB-D463C5396A79:12724c2f-cead-45ab-b1df-e8a3374543de" style="margin: 0px; padding: 0px; float: none; display: inline;">標籤: <a href="/tags/%e3%80%8cData+Science+%e5%88%b0%e5%ba%95%e6%98%af%e4%bb%80%e9%ba%bc%e5%be%9e%e4%b8%80%e5%80%8b%e5%ae%8c%e5%85%a8%e5%a4%96%e8%a1%8c%e8%a7%92%e5%ba%a6%e4%be%86%e7%9c%8b%e3%80%8d" rel="tag">「Data Science 到底是什麼從一個完全外行角度來看」</a>,<a href="/tags/data+science" rel="tag">data science</a>,<a href="/tags/hadoop" rel="tag">hadoop</a></div></section>