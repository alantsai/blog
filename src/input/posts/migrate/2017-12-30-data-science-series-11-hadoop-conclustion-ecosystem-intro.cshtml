Title: "[Data Science 到底是什麼從一個完全外行角度來看][11]Hadoop總結(上篇)–Ecosystem介紹"
Published: 2017-12-30
Modified: 2017-12-31
Image: /posts/migrate/2017-12-30-data-science-series-11-hadoop-conclustion-ecosystem-intro_Asset/image_thumb(70).png
Tags: ["hadoop","「data science 到底是什麼從一個完全外行角度來看」","data science"]
RedirectFrom: 2017/12/data-science-series-11-hadoop-conclustion-ecosystem-intro.html
Series: ["「Data Science 到底是什麼從一個完全外行角度來看」"]
---
<section><figure><a href="https://lh3.googleusercontent.com/-AFtZXWeRX9o/WkelcVZsOFI/AAAAAAAAXYo/12-GHKI4I0cZwt4XHEBZtrH2AKylW9veACHMYCw/s1600-h/image%255B2%255D"><img width="654" height="290" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-30-data-science-series-11-hadoop-conclustion-ecosystem-intro_Asset/image_thumb(70).png" border="0" class="img-responsive"></a><br><figcaption><span>圖片來源：                 <a href="https://pixabay.com/en/books-spine-colors-pastel-1099067/">https://pixabay.com/en/books-spine-colors-pastel-1099067/</a> 和                 <a href="https://pixabay.com/en/math-blackboard-education-classroom-1547018/">https://pixabay.com/en/math-blackboard-education-classroom-1547018/</a></span></figcaption></figure></section><section><p>在上一篇(<a href="http://blog.alantsai.net/2017/12/data-science-series-10-hadoop-streaming-intro-use-net-core-for-MapReduce.html">[10]用.Net Core跑Hadoop MapReduce - Streaming介紹</a>)透過Hadoop Streaming的幫助，改成用.Net Core 2.0的程式作為MapReduce的邏輯，基本上這個系列裡面的Hadoop介紹也要告一個段落。</p><p>這一篇，將會快速回顧到目前為止所了解到關於Hadoop的部分，再來介紹Hadoop Ecosystem，和3個比較常見的package介紹。         </p></section>         <section>             <a name="KMContentPageTopID" id="KMContentPageTopID"></a><div id="divKMOutline" style="border-style: groove none; margin: 10px 0px;"><ul style="margin: 0px 0px 0px 20px;"><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514643100960577" ;="">溫故知新</a><br></li><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514643100961193" ;="">Hadoop Ecosystem</a><br></li><ul style="margin: 0px 0px 0px 30px;"><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514643100961585" ;="">HIVE</a><br></li><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514643100961836" ;="">HBase</a><br></li><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514643100961587" ;="">Spark</a><br></li></ul><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514643100961168" ;="">結語</a><br></li></ul></div></section>          <a name="more"></a> <section><h2 id="WizKMOutline_1514643100960577">溫故知新</h2><p>先來快速回顧一下到目前為止看了什麼：         </p><ol><li>從Big Data開始介紹，了解到Big Data裡面的3個V - 由於別的技術的可行性提高，Big Data成長也越來越快：<a href="http://blog.alantsai.net/2017/12/data-science-series-03-big-data-intro.html">鏈接</a></li><li>再來介紹了Hadoop的誕生 - 為了因應Big Data而誕生的程式：<a href="http://blog.alantsai.net/2017/12/data-science-series-04-hadoop-intro.html">鏈接</a></li><li>在來手動用VM建立了一個pseudo-distributed mode的hadoop：<a href="http://blog.alantsai.net/2017/12/data-science-series-05-install-and-test-hadoop-part1.html">上篇鏈接</a>、<a href="http://blog.alantsai.net/2017/12/data-science-series-06-install-and-test-hadoop-part2.html">下篇鏈接</a></li><li>有了一個可運作的hadoop之後，了解了Hadoop的核心：YARN和MapReduce：<a href="http://blog.alantsai.net/2017/12/data-science-series-07-deeper-look-at-yarn-and-hdfs-in-hadoop.html">鏈接</a></li><li>                 接下來把Hadoop的VM複製出來用來建立一個worker - 打造了fully-distributed mode：<a href="http://blog.alantsai.net/2017/12/data-science-series-08-hadoop-fully-distributed-mode-tutorial.html">鏈接</a></li><li>                 最後介紹了MapReduce的程式，先介紹原理，後來介紹如何用.Net Core寫一個：<a href="http://blog.alantsai.net/2017/12/data-science-series-09-hadoop-map-reduce-java-wordcount-example.html">MapReduce原理</a>、<a href="http://blog.alantsai.net/2017/12/data-science-series-10-hadoop-streaming-intro-use-net-core-for-MapReduce.html">Hadoop Streaming執行.Net Core</a></li></ol><p>從目前了解的來看，對於整個Hadoop的核心已經有了基本的概念，馬上浮現的問題是，這樣建立和管理Hadoop不會太麻煩嗎？透過MapReduce寫不同的邏輯太難寫了吧。         </p><p>相信看完之後會有種感覺，<em><strong>Hadoop感覺很"底層"</strong></em>，從應用程度來說太不Friendly（友善），並且要怎麼管理這些Hadoop Cluster？         </p><div class="bs-callout bs-callout-info">題外話，Hadoop給我的感覺就像程式開發裡面的Assembly，速度快，但是不好寫，因此會使用High Level Language像C#。         </div><p>因此，接下來看看一些在Hadoop上面的Ecosystem和所謂的<em>Hadoop Distribution</em>。         </p></section><section><h2 id="WizKMOutline_1514643100961193">Hadoop Ecosystem</h2><p>還記得之前提到Hadoop版本的時候介紹了2.0之後其中一個大改變就是加入了YARN。         </p><p>YARN的加入讓在Hadoop上面開發變得更加的簡單，因此很多Application蓋在了Hadoop的上面，達到利用Hadoop的Cluster運算和HDFS。         </p><p>下圖是一個Hadoop Ecosystem的幾個重要package，這邊會針對裡面的：             </p><ul><li>HIVE - 用類似sql語法執行MapReduce的方式</li><li>HBase - 在HDFS上面的NoSql儲存</li><li>Spark - 另外一種執行工作的模式</li></ul>多做一些簡單介紹             <figure><figcaption><a href="https://lh3.googleusercontent.com/-8dp6YCqVs1A/WkelffWeKRI/AAAAAAAAXYw/ry1UcQ7s4EYTTS75O3B8BSLAbiTtUACIwCHMYCw/s1600-h/image%255B7%255D"><img width="654" height="316" title="image" style="display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-30-data-science-series-11-hadoop-conclustion-ecosystem-intro_Asset/image_thumb[3](48).png" border="0" class="img-responsive"></a>Hadoop Ecosystem，來源：http://www.dotnettricks.com/learn/hadoop/apache-hadoop-ecosystem-and-components</figcaption></figure><section><h3 id="WizKMOutline_1514643100961585">HIVE</h3><figure><a href="https://lh3.googleusercontent.com/-87yddDTmLxY/Wkelh3iVasI/AAAAAAAAXY4/iG6ORs8ShQkz6Sh-s_L2b3Q0MwDII3CawCHMYCw/s1600-h/image%255B10%255D"><img width="269" height="161" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-30-data-science-series-11-hadoop-conclustion-ecosystem-intro_Asset/image_thumb[4](48).png" border="0" class="img-responsive"></a><br><figcaption>Hive Logo</figcaption></figure><p>HIVE一開始是由Facebook開發，後來捐出來變成一個Open Source專案。主要目的是讓懂SQL的人能夠快速上手能從HDFS取資料。             </p><p>還記得之前提到要寫MapReduce感覺有點麻煩，HIVE其實就是一個使用<em>類似SQL語法的語言，HiveQL來和MapReduce溝通的中間層</em>。             </p><p>換句話說，用HiveQL語法，HIVE會自動轉成MapReduce從HDFS裡面把資料取出來。             </p><p>因此，懂SQL就能夠快速上手從HDFS裡面撈出資料。             </p><p>下面是從<a href="https://en.wikipedia.org/wiki/Apache_Hive">Wikipedia</a>截取的WordCount範例，可以看到，基本上會sql基本上看的懂在做什麼，把這個和Java和.Net Core版本做比較，對於會sql的人來說這個簡單多了                 </p><pre class="brush: plain;"><code class="language-none line-numbers">DROP TABLE IF EXISTS docs;
CREATE TABLE docs (line STRING);
LOAD DATA INPATH 'input_file' OVERWRITE INTO TABLE docs;
CREATE TABLE word_counts AS
SELECT word, count(1) AS count FROM
 (SELECT explode(split(line, '\s')) AS word FROM docs) temp
GROUP BY word
ORDER BY word;
                </code></pre><p>更多資訊，請參考：<a href="https://hive.apache.org/">官網</a></p></section><section><h3 id="WizKMOutline_1514643100961836">HBase</h3><figure><a href="https://lh3.googleusercontent.com/-NA8UMRxLdFo/Wkelj6ivn5I/AAAAAAAAXZA/uZ_esDB39t0fqEQTXFWRXISGuqnX5SKFQCHMYCw/s1600-h/image%255B13%255D"><img width="654" height="174" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-30-data-science-series-11-hadoop-conclustion-ecosystem-intro_Asset/image_thumb[5](33).png" border="0" class="img-responsive"></a><br><figcaption>HBase logo</figcaption></figure><p>聽到Big Data最長聽到的就是No Sql database，也就是不像sql先定義好每個table有什麼欄位的另外一種Database。             </p><p>HBase就是一個<em>使用HDFS的No Sql Database</em>。             </p><p>HBase還提供吧資料儲存在Memory達到<em>快速讀取</em>HDFS資料的一個界面，更好的能夠和其他package結合，例如<em>可以用HIVE去撈HBase的資料</em>。             </p><p>因為HBase速度的關係，有些會把cold data（封存用的資料少存取）放到<em>HDFS</em>裡面，而hot data（長存取的資料）放到<em>HBase</em>裡面。             </p><div class="bs-callout bs-callout-info">題外話，HBase可以不架在YARN上面，只需要HDFS即可。             </div><p>更多資料請參考：<a href="https://hbase.apache.org/">HBase</a></p></section><section><h3 id="WizKMOutline_1514643100961587">Spark</h3><figure><a href="https://lh3.googleusercontent.com/-gntY9dVsVRk/Wkell4L4KzI/AAAAAAAAXZI/OFQ2u5GRA181CpaLWK1FljrxxYp1-UrGgCHMYCw/s1600-h/image%255B16%255D"><img width="310" height="176" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-30-data-science-series-11-hadoop-conclustion-ecosystem-intro_Asset/image_thumb[6](32).png" border="0" class="img-responsive"></a><br><figcaption>spark logo</figcaption></figure><p>基本上Spark是整個ecosystem最夯的部分，因為他解決了Hadoop的一個很致命的問題，<em>過慢問題</em>，官網說在最快的情況可以比Hadoop快<em>100倍</em>。             </p><p>Spark的基本概念很簡單，以Hadoop來說，每一個階段的執行，都會把結果<em>儲存在HDFS</em>，換句話說很多<em>IO操作</em>。             </p><p>Spark用了另外一個方式，前面一段output會是後面一段的input - 這個的儲存<em>都是在Memory</em>，換句話說減少了IO量，提升了速度。             </p><div class="bs-callout bs-callout-info">或許會說，不是都cluster了嗎，怎麼還會慢？這邊的慢指的是相對來說。要記得，這邊處理都是<em>ZB等級</em>的資料，真的跑下去還是很花時間。             </div><p>更多資料請參考：<a href="https://spark.apache.org/">官網</a></p></section><p>從Ecosystem的圖可以看到其實還有很多重要的package，而看到這個也會發現Hadoop真的變成了一個<em>很底層的核心</em>。         </p><p>看到了這麼多package了之後，有個問題會浮現出來，這麼多不同package要自己安裝不是很麻煩嗎？更別說package之間可能還有相容性問題，難道沒有一個一整包包好的服務嗎？還有cluster management怎麼辦？雖然有package做整件事情，但是怎麼整合呢？         </p><p>這也是所謂的<em>Hadoop Distribution</em></p></section><section><h2 id="WizKMOutline_1514643100961168">結語</h2><p>這篇介紹了整個Hadoop的Ecosystem，並且挑出了3個最夯的：Hive、HBase和Spark。                 </p><p>本來這篇要把整個總結寫完，但是發現篇幅有點長，因此在下一篇在介紹Hadoop Distribution和相關的工作頭銜，Data Engineer介紹。                 </p></section>  <section>     <div class="wlWriterEditableSmartContent" id="scid:77ECF5F8-D252-44F5-B4EB-D463C5396A79:f10ae1af-a21d-42e3-be68-5a462692d1ea" style="margin: 0px; padding: 0px; float: none; display: inline;">標籤: <a href="/tags/%e3%80%8cData+Science+%e5%88%b0%e5%ba%95%e6%98%af%e4%bb%80%e9%ba%bc%e5%be%9e%e4%b8%80%e5%80%8b%e5%ae%8c%e5%85%a8%e5%a4%96%e8%a1%8c%e8%a7%92%e5%ba%a6%e4%be%86%e7%9c%8b%e3%80%8d" rel="tag">「Data Science 到底是什麼從一個完全外行角度來看」</a>,<a href="/tags/data+science" rel="tag">data science</a>,<a href="/tags/hadoop" rel="tag">hadoop</a></div></section>