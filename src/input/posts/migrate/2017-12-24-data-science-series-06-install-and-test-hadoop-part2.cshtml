Title: "[Data Science 到底是什麼從一個完全外行角度來看][06]建立Hadoop環境 -下篇"
Published: 2017-12-24
Modified: 2017-12-25
Image: /posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb(65).png
Tags: ["hadoop","「data science 到底是什麼從一個完全外行角度來看」","data science"]
RedirectFrom: 2017/12/data-science-series-06-install-and-test-hadoop-part2.html
Series: ["「Data Science 到底是什麼從一個完全外行角度來看」"]
---
<section><figure><a href="https://lh3.googleusercontent.com/-CJ0uihAhLFo/Wj74iyCdZTI/AAAAAAAAXOs/EYVw6iPpjkMYdw6v2RB52eIX-QEZrCqGgCHMYCw/s1600-h/image%255B2%255D"><img width="654" height="290" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb(65).png" border="0" class="img-responsive"></a><br><figcaption><span>圖片來源：<a href="https://pixabay.com/en/books-spine-colors-pastel-1099067/">https://pixabay.com/en/books-spine-colors-pastel-1099067/</a> 和 <a href="https://pixabay.com/en/math-blackboard-education-classroom-1547018/">https://pixabay.com/en/math-blackboard-education-classroom-1547018/</a></span></figcaption><br></figure></section><section><p>上一篇（<a href="http://blog.alantsai.net/2017/12/data-science-series-05-install-and-test-hadoop-part1.html">[05]建立Hadoop環境 -上篇</a>）透過VMWare Player把Ubuntu裝好並且一些相關環境設定到，等於把hadoop的基礎環境建立好了。   </p><p>這篇將延續上篇的環境，把Hadoop建立上去，並且讓Hadoop跑一個hello world的範例。   </p></section>   <section>   <a name="KMContentPageTopID" id="KMContentPageTopID"></a><div id="divKMOutline" style="border-style: groove none; margin: 10px 0px;"><ul style="margin: 0px 0px 0px 20px;"><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514075612638700" ;="">環境準備</a><br></li><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514075612638645" ;="">建立Hadoop測試環境</a><br></li><ul style="margin: 0px 0px 0px 30px;"><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514075612638964" ;="">安裝和設定Hadoop</a><br></li><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514075612638833" ;="">測試Hadoop</a><br></li></ul><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514075612638738" ;="">執行Hadoop的Hello World - WordCount</a><br></li><li><a style="line-height: 1.6; font-size: 15px;" href="#WizKMOutline_1514075612638726" ;="">結語</a><br></li></ul></div></section>   <a name="more"></a>   <section><h2 id="WizKMOutline_1514075612638700">環境準備</h2><p>這邊的清單和上一篇一樣，如果上篇已經有抓過，可以跳過：   </p><dl><dt>     主機環境    </dt><dd>     接下來使用到的機器規格如下：     <ul><li>OS - Windows 10 1703</li><li>CPU - i7-6500U 雙核</li><li>Memory - 16GB</li></ul></dd><dt>     VMWare Player 14    </dt><dd><p>任何虛擬機器軟體都可以，只是剛好用的是VMWare Player 14。    </p><ul><li><a href="https://my.vmware.com/en/web/vmware/free#desktop_end_user_computing/vmware_workstation_player/14_0|PLAYER-1410|product_downloads">下載頁面</a></li><li>檔案大小約 90MB </li></ul></dd><dt>     Ubuntu 16.04.3    </dt><dd><p>其他版本的Ubuntu也沒問題 - 如果用的是Ubuntu 14，那麼只有等一下安裝openjdk的部分會有問題，其他都一樣。     </p><ul><li><a href="https://www.ubuntu.com/download/desktop">下載頁面</a></li><li><a href="https://www.ubuntu.com/download/desktop/thank-you?version=16.04.3&amp;architecture=amd64">直接下載（約1.4GB）</a></li></ul></dd><dt>     Hadoop v2.7.4    </dt><dd><p>基本上 v2.x 的都沒有問題，只是剛好手上有2.7.4所以沒有在下載新的。如果是v3.0那麼設定會不同     </p><ul><li><a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz">下載頁面</a></li><li><a href="http://apache.stu.edu.tw/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz">直接下載（約254 MB）</a></li></ul></dd><dt>     MapReduce的Hello World程式 - WordCount    </dt><dd><p>這個是用來測試map reduce的hello world程式：     </p><ul><li><a href="https://1drv.ms/u/s!Am-7OXCiIo1Lh9EaLbZsQyArJyYZ6Q">WordCount2.jar</a></li><li><a href="https://1drv.ms/t/s!Am-7OXCiIo1Lh9EbxkNiZl1d7ERYtQ">jane_austen.txt</a> - pride and prejudice 前三章 - 測試算字數用</li></ul></dd></dl></section><section><h2 id="WizKMOutline_1514075612638645">建立Hadoop測試環境</h2><p>基本上整個的環境建立大概可以分幾個部分：   </p><ol><li>安裝Ubuntu VM</li><li>設定Ubuntu環境</li><li>安裝和設定Hadoop</li><li>測試Hadoop</li></ol><div class="bs-callout bs-callout-info">這篇會介紹<em>第三步和第四部</em>的部分   </div><section><h3 id="WizKMOutline_1514075612638964">安裝和設定Hadoop</h3><dl><dt>      下載和解壓縮Hadoop     </dt><dd><p>先用firefox下載（<a href="http://apache.stu.edu.tw/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz">直接下載</a>）hadoop到<em><strong>Downloads</strong></em>資料夾      </p><figure><a href="https://lh3.googleusercontent.com/-oweprqnd6Dc/Wj74lGMOExI/AAAAAAAAXO0/vmKK4rpueWgiiORjsVeTK-EhlQ_uzr9WACHMYCw/s1600-h/image%255B5%255D"><img width="518" height="461" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb[1](57).png" border="0" class="img-responsive"></a> <figcaption>下載最後位置</figcaption></figure><p>在Terminal（快速鍵 <code>Ctrl + Alt + t</code>）裡面執行以下指令：      </p><pre class="brush: plain;"><code class="language-none line-numbers">cd Downloads
sudo tar -zxvf ./hadoop-2.7.4.tar.gz -C /usr/local
cd /usr/local
sudo mv ./hadoop-2.7.4/ ./hadoop
sudo addgroup hadoop
sudo chown -R hduser:hadoop hadoop</code></pre>這個的作用是把它解壓縮出來，放到<code>/usr/local/hadoop</code>的位置，並且設定執行權限      <figure><a href="https://lh3.googleusercontent.com/-ZKp5Roo5qng/Wj74niNqNAI/AAAAAAAAXO8/HqSjgT_mEk8j-Oc4z5YXxcR1sBYV6JDQACHMYCw/s1600-h/image%255B8%255D"><img width="654" height="366" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb[2](47).png" border="0" class="img-responsive"></a> <figcaption>解壓縮完成看到hadoop資料夾</figcaption></figure></dd><dt>      設定 hadoop/etc/hadoop/core-site.xml     </dt><dd><p>用Terminal執行：<code>gedit /usr/local/hadoop/etc/hadoop/core-site.xml</code></p><p>在 <code>Configuration</code>裡面輸入：      </p><pre class="brush: xml;"><code class="language-markup line-numbers">&lt;property&gt;
&lt;name&gt;fs.default.name&lt;/name&gt;
&lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
&lt;/property&gt;</code></pre><p>這個是在設定<em><strong>NameNode</strong></em>位置在哪裡 - NameNode之後會介紹，但是基本上就是主控HDFS的Master。      </p><figure><a href="https://lh3.googleusercontent.com/-AWpG-rty1OI/Wj74qs3Zc7I/AAAAAAAAXPE/EhFHm3KeSGwBqSrqCr-HrqGtGlDaK1LrQCHMYCw/s1600-h/image%255B11%255D"><img width="654" height="674" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb[3](44).png" border="0" class="img-responsive"></a> <figcaption>修改core-site.xml的截圖</figcaption></figure></dd><dt>      修改hadoop-env.sh     </dt><dd><p>這邊要把<code>${JAVA_HOME}</code>的值寫進去（理論上應該不需要才對，因為我們之前有設定參數，但是好像吃不進去，所以要寫死進去）      </p><p>在Terminal執行：<code>gedit /usr/local/hadoop/etc/hadoop/hadoop-env.sh</code></p><p>找到：<code>export JAVA_HOME=${JAVA_HOME}</code>然後把它改成<code>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</code></p><figure><a href="https://lh3.googleusercontent.com/-9Lq7ai8kwRc/Wj74t2heJsI/AAAAAAAAXPM/1U1JZaaWG8IVqTejPWjgNG4Zo0GjTK7QQCHMYCw/s1600-h/image%255B14%255D"><img width="547" height="400" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb[4](44).png" border="0" class="img-responsive"></a> <figcaption>修改之後的結果</figcaption></figure></dd><dt>      設定hdfs-site.xml     </dt><dd><p>這邊設定的是：      </p><ul><li>每一個在HDFS的檔案要replicate幾份 - 預設都是3</li><li>NameNode儲存位置</li><li>DataNode儲存位置</li></ul><p>在Terminal執行：<code>gedit /usr/local/hadoop/etc/hadoop/hdfs-site.xml</code>，然後在<code>Configuration</code>裡面加入： </p><pre class="brush: xml;"><code class="language-markup line-numbers">&lt;property&gt;
 &lt;name&gt;dfs.replication&lt;/name&gt;
 &lt;value&gt;3&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
 &lt;value&gt; file:/usr/local/hadoop/hadoop_data/hdfs/namenode&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
 &lt;value&gt; file:/usr/local/hadoop/hadoop_data/hdfs/datanode&lt;/value&gt;
&lt;/property</code></pre><figure><a href="https://lh3.googleusercontent.com/-cA4L9F97mac/Wj74yxoopqI/AAAAAAAAXPU/SP2aJt2wKkIDlMIg0JxzeXi6gASXT0pSACHMYCw/s1600-h/image%255B17%255D"><img width="654" height="737" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb[5](30).png" border="0" class="img-responsive"></a> <figcaption>修改畫面</figcaption></figure></dd><dt>      修改yarn-site.xml     </dt><dd><p>這邊修改的是yarn的設定，在Terminal執行<code>gedit /usr/local/hadoop/etc/hadoop/yarn-site.xml</code>，在<code>Configuration</code>裡面加入：      </p><pre class="brush: xml;"><code class="language-markup line-numbers">&lt;property&gt;
 &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
 &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
 &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;
 &lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
 &lt;value&gt;2048&lt;/value&gt;
&lt;/property&gt;</code></pre><p>這邊後面兩個，<code>yarn.nodemanager.resource.cpu-vcores</code> 和 <code>yarn.nodemanager.resource.memory-mb</code> 是設定使用到的資源，如果後面執行不太起來要注意這個值和VM給的資源。      </p><figure><a href="https://lh3.googleusercontent.com/-2PEhHzm6Mr0/Wj745LhaZcI/AAAAAAAAXPc/gG79UvtJrm4Jgo6iErNHMzg3PYXXnk7jQCHMYCw/s1600-h/image%255B20%255D"><img width="654" height="737" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb[6](28).png" border="0" class="img-responsive"></a> <figcaption>修改完的畫面</figcaption></figure></dd><dt>      修改marped-site.xml     </dt><dd><p>這個檔案預設不存在，所以要從template把它先復製出來。       </p><p>在Terminal輸入：       </p><pre class="brush: plain;"><code class="language-none line-numbers">cd /usr/local/hadoop/etc/hadoop
sudo cp mapred-site.xml.template mapred-site.xml
cd ~
sudo gedit /usr/local/hadoop/etc/hadoop/mapred-site.xml</code></pre><p>打開了之後，把<code>configuration</code>改成：       </p><pre class="brush: xml;"><code class="language-markup line-numbers">&lt;property&gt;
 &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
 &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;</code></pre><figure><a href="https://lh3.googleusercontent.com/-opld-kD0iIw/Wj74-ErWZ4I/AAAAAAAAXPk/PvEgkhtVQBM89YE-06j6QMwlqugaFHw-gCHMYCw/s1600-h/image%255B23%255D"><img width="560" height="513" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb[7](22).png" border="0" class="img-responsive"></a> <figcaption>完成設定</figcaption></figure></dd><dt>      建立HDFS用到的目錄     </dt><dd>      在Terminal輸入：      <pre class="brush: plain;"><code class="language-none line-numbers">sudo rm -rf /usr/local/hadoop/hadoop_data/hdfs
mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode
mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode
sudo chown -R hduser:hduser /usr/local/hadoop</code></pre><p>這個將會建立hdfs的相關資料夾      </p></dd></dl></section><section><h3 id="WizKMOutline_1514075612638833">測試Hadoop</h3><p>以上就是hadoop的安裝和設定，接下來只需要把它run起來即可。    </p><dl><dt>      格式化HDFS     </dt><dd>      在Terminal執行：      <pre class="brush: plain;"><code class="language-none line-numbers">hadoop namenode -format
hadoop datanode -format </code></pre><div class="bs-callout bs-callout-warning">中間可能會問是否確定繼續執行，記得要輸入<code>yes</code></div></dd><dt>      啟動yarn和hdfs     </dt><dd>      在Terminal輸入：      <pre class="brush: plain;"><code class="language-none line-numbers">start-yarn.sh
start-dfs.sh</code></pre><p>這個是在<em>Master上面</em>執行，會自動透過ssh的方式把所有Slave也一起啟動。      </p><div class="bs-callout bs-callout-info">還有兩種啟動方式：       <ul><li><code>start-all.sh</code> 和 <code>stop-all.sh</code> - 這個已經被deprecated 不過同等於上面兩個在一起執行</li><li><code>hadoop-daemon.sh namenode/datanode</code> 和 <code>yarn-deamon.sh resourcemanager</code> - 這個是手動在<em>各個</em>節點裡面手動啟動對應服務</li></ul></div></dd><dt>      確認啟動process是否正常     </dt><dd><p>在Terminal上面執行：<code>jps</code></p><figure><a href="https://lh3.googleusercontent.com/-DKYiOi2CdwI/Wj75EU8N6JI/AAAAAAAAXP0/pns4S7r4W9U5r-5XYK--2Lm0syBb2AIHACHMYCw/s1600-h/image%255B26%255D"><img width="295" height="132" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb[8](20).png" border="0" class="img-responsive"></a> <figcaption>檢查執行的服務</figcaption></figure><p>這邊會看到5個服務：      </p><ol><li>NameNode</li><li>SecondaryNameNode</li><li>ResourceManager</li><li>NodeManager</li><li>DataNode</li></ol><div class="bs-callout bs-callout-info">在真的分散式架構，前3個只會在Master出現，後面兩個只會在Slave出現      </div></dd><dt>      確認Web UI是否正常     </dt><dd><p>服務啟動成功之後可以在Firefox輸入：      </p><ul><li><a href="http://localhost:8088">http://localhost:8088</a> - 這個是ResourceManager的web界面</li><li><a href="http://localhost:50070">http://localhost:50070</a>- 這個是NameNode的web界面 - 換句話說是hdfs的畫面</li></ul><figure><a href="https://lh3.googleusercontent.com/-CxdC5sF6C9M/Wj75HOT5EII/AAAAAAAAXP8/v97Z3G6HdvkJxItq2BajlUucD7nD6HoewCHMYCw/s1600-h/image%255B29%255D"><img width="654" height="332" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb[9](10).png" border="0" class="img-responsive"></a> <figcaption>ResourceManager的畫面</figcaption></figure><figure><a href="https://lh3.googleusercontent.com/-R_tSLQgS1-8/Wj75KtkmNqI/AAAAAAAAXQE/XllmWKDY8B8mZvqQStAwWt9muDHar9jZQCHMYCw/s1600-h/image%255B32%255D"><img width="654" height="412" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb[10](8).png" border="0" class="img-responsive"></a> <figcaption>NameNode的畫面</figcaption></figure></dd></dl></section></section><section><h2 id="WizKMOutline_1514075612638738">執行Hadoop的Hello World - WordCount</h2><p>首先先把 <a href="https://1drv.ms/u/s!Am-7OXCiIo1Lh9EaLbZsQyArJyYZ6Q">WordCount2.jar</a>和<a href="https://1drv.ms/t/s!Am-7OXCiIo1Lh9EbxkNiZl1d7ERYtQ">jane_austen.txt</a>下載到<code>Downloads</code>裡面。   </p><figure><a href="https://lh3.googleusercontent.com/-7xSrWxuljkM/Wj75QraE9TI/AAAAAAAAXQM/wa9euY0Zc64aN5fwPS0qEUHgwt2BNUnswCHMYCw/s1600-h/image%255B35%255D"><img width="654" height="256" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb[11](11).png" border="0" class="img-responsive"></a><br><figcaption>下載完的畫面</figcaption></figure><p>把檔案複製到 hadoop的HDFS裡面，在Terminal輸入： </p><pre class="brush: plain;"><code class="language-none line-numbers">cd ~/Downloads
hadoop fs -mkdir -p /user/hduser/input
hadoop fs -copyFromLocal jane_austen.txt /user/hduser/input</code></pre><div class="bs-callout bs-callout-info">可以用<code>hadoop fs -ls /user/hduser/inpu</code>檢查複製進去的檔案。   </div><p>執行WordCount的程式，<code>hadoop jar wordcount2.jar WordCount /user/hduser/input/jane_austen.txt /user/hduser/output</code></p><figure><a href="https://lh3.googleusercontent.com/-T2PAMILtwSU/Wj75T-Gw2bI/AAAAAAAAXQU/i1hd2owz9HcRqJhHHn5cwxR_doTQXDQhACHMYCw/s1600-h/image%255B38%255D"><img width="654" height="138" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb[12](13).png" border="0" class="img-responsive"></a><br><figcaption>執行WordCount</figcaption></figure><p>檢查執行結果：<code>hadoop fs -cat /user/hduser/output/part-r-00000</code></p><figure><a href="https://lh3.googleusercontent.com/-_QFg6YIxN2A/Wj75WHNveAI/AAAAAAAAXQc/isF8Pa2HIdI2UXqH5xhJBXz1IRGfuVVsQCHMYCw/s1600-h/image%255B41%255D"><img width="654" height="296" title="image" style="margin: 0px; display: inline; background-image: none;" alt="image" src="/posts/migrate/2017-12-24-data-science-series-06-install-and-test-hadoop-part2_Asset/image_thumb[13](11).png" border="0" class="img-responsive"></a><br><figcaption>看到最後計算結果</figcaption></figure><div class="bs-callout bs-callout-warning">如果要在執行一次計算，需要先把hdfs裡面的output<em>砍掉</em>，要不然會執行不了。指令是：<code>hadoop fs -rm -r /user/hduser/output</code></div><div class="bs-callout bs-callout-warning">如果執行有問題，或者run不起來，可以試試重開機，然後從<em>測試Hadoop</em>裡面的<em>格式化HDFS</em>開始重新做一次。   </div></section><section><h2 id="WizKMOutline_1514075612638726">結語</h2><p>在這篇把整個Hadoop建立完成，並且執行了一個map reduce的word count程式計算出pride and prejudice前3章的字數計算。   </p><p>在這篇建立出來的hadoop是所謂的<em><strong>pseudo-distributed mode</strong></em>，換句話說Master和Slave在<em><strong>同一台機器</strong></em>，但是實際運作上會有Master對上多個Slave。   </p><p>不過在進入這種分散式模式之前，需要在了解一些hadoop細節。   </p><p>在下一篇(<a href="http://blog.alantsai.net/2017/12/data-science-series-07-deeper-look-at-yarn-and-hdfs-in-hadoop.html">[07]更深入看看Hadoop裡面的YARN和HDFS</a>)，將會在針對hadoop裡面的分散式模式在做更詳細一點介紹，包含yarn和hdfs裡面對應的process。   </p></section><section>&nbsp;&nbsp; <div class="wlWriterEditableSmartContent" id="scid:77ECF5F8-D252-44F5-B4EB-D463C5396A79:72592fa7-81a9-47e5-b1d2-ba2f92822336" style="margin: 0px; padding: 0px; float: none; display: inline;">標籤: <a href="/tags/%e3%80%8cData+Science+%e5%88%b0%e5%ba%95%e6%98%af%e4%bb%80%e9%ba%bc%e5%be%9e%e4%b8%80%e5%80%8b%e5%ae%8c%e5%85%a8%e5%a4%96%e8%a1%8c%e8%a7%92%e5%ba%a6%e4%be%86%e7%9c%8b%e3%80%8d" rel="tag">「Data Science 到底是什麼從一個完全外行角度來看」</a>,<a href="/tags/data+science" rel="tag">data science</a>,<a href="/tags/hadoop" rel="tag">hadoop</a></div></section>